#!/usr/bin/env python
"""
Code to measure performance on GPU vs CPU for executing a convnet with ResNet blocks code.

This is a modified version of my code located at:
https://github.com/hideyukiinada/cifar10/blob/master/project/keras_25
which accomplished 91.93% against CIFAR10 dataset.

I ran 100 epochs for the CIFAR10 accuracy experiment, but for this benchmark, I am only running a single batch
without data augmentation so that it can measure performance on CPU.
Therefore accuracy measuring code has been also removed.

__author__ = "Hide Inada"
__copyright__ = "Copyright 2018, Hide Inada"
__license__ = "The MIT License"
__email__ = "hideyuki@gmail.com"
"""

import os
import logging

import numpy as np
import sys
import time

import keras
import keras.layers
from keras.models import Model
from keras.layers import Input
from keras.layers import Conv2D
from keras.layers import Dense
from keras.layers import BatchNormalization
from keras.layers import GlobalAveragePooling2D
from keras.layers import ReLU
from keras.optimizers import Adam

log = logging.getLogger(__name__)
logging.basicConfig(level=os.environ.get("LOGLEVEL", "INFO"))  # Change the 2nd arg to INFO to suppress debug logging

EPOCHS = 1
BATCH_SIZE = 64
NUM_TESTS = 4


def single_conv_block(tens, filters=None, kernel_size=None, strides=None):
    tens = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='SAME')(tens)
    tens = BatchNormalization()(tens)
    tens = ReLU()(tens)

    return tens


def single_conv_block_no_activation(tens, filters=None, kernel_size=None, strides=None):
    tens = Conv2D(filters=filters, kernel_size=kernel_size, strides=strides, padding='SAME')(tens)
    tens = BatchNormalization()(tens)

    return tens


def single_resnet_block(tens, filters=None, kernel_size=None, downsample=False):
    original_tens = tens

    if downsample:
        original_tens = single_conv_block_no_activation(tens, filters=filters, kernel_size=1, strides=2)
        tens = single_conv_block(tens, filters=filters, kernel_size=kernel_size, strides=2)
    else:
        tens = single_conv_block(tens, filters=filters, kernel_size=kernel_size, strides=1)

    tens = single_conv_block_no_activation(tens, filters=filters, kernel_size=kernel_size, strides=1)

    tens = keras.layers.add([tens, original_tens])

    tens = ReLU()(tens)

    return tens


def n_resnet_blocks(num_blocks, tens, filters_prev=None, filters=None, kernel_size=None, downsample=False):
    if downsample:
        tens = single_resnet_block(tens, filters=filters, kernel_size=kernel_size, downsample=True)
        for i in range(num_blocks - 1):
            tens = single_resnet_block(tens, filters=filters, kernel_size=kernel_size)
    else:
        for i in range(num_blocks):
            tens = single_resnet_block(tens, filters=filters, kernel_size=kernel_size)

    return tens


def train_and_predict():
    """Train the model and predict.

    This file downloads dataset via Keras util.
    """

    (x, y), (x_test, y_test) = keras.datasets.cifar10.load_data()

    # Change the value from 0<= x <= 255 in UINT8 to 0 <= x <= 1 in float
    x = x / 255.0
    x_test = x_test / 255.0

    y_oh = keras.utils.to_categorical(y, 10) * 1.0
    y_test_oh = keras.utils.to_categorical(y_test, 10)

    input_tens = Input(shape=(32, 32, 3))

    tens = single_conv_block(input_tens, filters=64, kernel_size=3, strides=1)

    tens = n_resnet_blocks(10, tens, filters=64, kernel_size=3)
    tens = n_resnet_blocks(10, tens, filters=128, kernel_size=3, downsample=True)
    tens = n_resnet_blocks(10, tens, filters=256, kernel_size=3, downsample=True)

    tens = GlobalAveragePooling2D()(tens)

    last_tens = Dense(10, activation='softmax')(tens)

    model = Model(input_tens, last_tens)
    adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999)
    model.compile(adam, loss=keras.losses.categorical_crossentropy, metrics=[keras.metrics.categorical_accuracy])

    print("Training start")
    for i in range(NUM_TESTS):
        # Per Python doc, process_time() is fractional seconds of the sum of the system and user CPU time of
        # the current process.  Python 3.3 is required.
        start_time = time.process_time()
        start_wall_time = time.time()
        # Just run a single batch as it will take too long to run an epoch on CPU
        model.train_on_batch(x[0:BATCH_SIZE, :, :, :], y_oh[0:BATCH_SIZE, :])
        end_time = time.process_time()
        end_wall_time = time.time()
        elapsed_time = end_time - start_time
        elapsed_wall_time = end_wall_time - start_wall_time
        print("[%d] Time elapsed:Process time: %f seconds. Wall time: %f seconds" % (i, elapsed_time, elapsed_wall_time))


def main():
    """Defines an application's main functionality"""
    train_and_predict()


if __name__ == "__main__":
    main()
